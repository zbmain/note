- **CV领域的几大任务**：
  - 图像分类
  - 目标检测
  - 目标分割
  - 目标追踪
  - 图像标注
- TensorFlow安装2.3.0的版本，根据自身机器的条件选择安装CPU版本或者GPU版本
- TensorFlow中重要的数据结构
  - **张量Tensor**：
    - 与numpy中的nd-array类似，只不过在TensorFlow中通过Tensor对象进行了封装
  - **TensorFlow日志提醒**：
    - I：information，通知信息
    - W：warnning，警告
    - E：Error，程序 出现错误
    - 屏蔽不重要的日志信息：
      - import os
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
  - 将张量转换成numpy中的nd-array：
    - np.array(tensor)
    - tensor.numpy()
  - **TensorFlow中的变量Variable操作**
    - 是一个特殊张量
    - 它里面对应的数据是可以被修改的，注意形状不能被修改
    - 使用场景：一般在模型中定义参数时使用Variable去定义，保证在模型的训练过程中能被调节
  - **TensorFlow中常用常见的一些模块**：
    - activations：激活函数
    - applications：一些常用的预训练模型
    - callbacks：在模型训练过程中被调用的一些方法
    - datasets：数据集的封装和处理
    - layers：网络层
    - losses：损失函数
    - metrics：指定评估模型的指标
    - models：抽象类，用来构建模型
    - optimizers：优化器，模型优化的方法指定
    - preprocessing：数据预处理
    - regularizers：正则化方法
    - utils：其它辅助功能
  - 鸢尾花数据集案例
    - 使用机器学习的方法和深度学习的方法做了一个流程的对比



- **深度学习与机器学习之间的区别和联系**

  - 深度学习是机器学习的一个发展方向，到目前为止发展最好的
  - 机器学习做数据的预处理可能需要一些专业的知识，在深度学习里面，不用人工去做这些数据的预处理，深度学习模型特别是卷积神经网络能够自动提取特征，正式因为这样的操作，导致**深度学习模型可解释性差，是一个黑盒操作**

- **神经网络**：

  - 是一个仿照人类的神经系统用来表示数据的计算的方式的模型

    - **单元结构**：
      - 神经元
        - 可以看作是简单的线性回归和激活函数操作
    - **网络结构**：
      - 输入层：负责输入样本特征
      - 隐藏层：完成一些特征的提取等计算操作
        - 卷积层
        - 激活层
        - 池化层
      - 输出层（全连接层）：负责输出模型的计算结果
    - 注意：每个神经元之间的连线代表一个权重

  - 激活函数

    - 引入非线性激活函数的目的就是让深度学习模型能够在样本线性不可分的情况下达到好的效果

      - Sigmoid

        - 处处可导，关于点（0,0.5）对称
        - 缺点：在函数的两侧容易造成梯度消失
        - 一般用作输出层（全连接层）的激活函数
        - 它的导数：f(x) (1 - f(x))
          - 导数的取值范围：(0, 1/4]

      - tanh：

        - 处处可导，关于原点对称，值域是（-1， 1）
        - 缺点：在函数的两侧容易造成梯度消失
        - 优点：相对于Sigmoid函数来说，能更快的让模型收敛
        - 它的导数：1 -  （g(x)）^2
          - 导数的取值范围：（0，1]

      - Relu(rectified linear unit):

        - 现阶段使用的最为广泛的一种激活函数
        - 缓解了Sigmoid和tanh中出现的梯度消失的问题
        - 计算量更小，反向传播时，后一层的梯度可以基本上无损传播到前一层，防止过拟合
        - 它的导数：
          - 输入如果大于0，导数值为1
          - 输入如果小于0，导数值为0

      - leaky Relu:

        - 在RELU的基础上做了一些修改
        - 如果训练深层神经网络模型的时，使用RELU达不到更好的效果，可以尝试使用它

      - softmax：

        - 应用场景：一般用在输出层的不同类别概率结果输出

      - 激活函数的选择：

        - 隐藏层：一般优先选择使用RELU，如果效果不理想，在去选择使用LeakyRELU，缺陷是可能会导致大批量的神经元死亡，我们一般可以去初始化一个较小的学习率去缓解这个问题
        - 输出层：
          - 分类任务：
            - 二分类：使用Sigmoid
            - 多分类：使用Softmax
            - 回归问题：使用恒等映射 y = x

      - 深度学习模型的参数初始化的意义及常用方式

        - 意义：让模型快速收敛

        - 常用的方式：

          - 1、随机初始化：使用标准正态分布随机出来的数值来初始化参数

          - 2、注意：深度学习里面，参数一般指：权重、偏置、卷积核的模板中的权重

          - 3、Xavier初始化（Glorot初始化）

            /1、正态化初始化：tf.keras.initializers.glorot_normal()

            - 特点：它从以 0 为中心，标准差为 `stddev = sqrt(2 / (fan_in + fan_out))` 的正态分布中抽取样本
            - 2、标准化Xavier初始化：tf.keras.initializers.glorot_uniform()
            - 特点：从 [-limit，limit] 中的均匀分布中抽取样本， 其中 `limit` 是 `sqrt(6 / (fan_in + fan_out))`
            - 论文中指出这样的初始化方法配合tanh和softsign激活函数使用效果更好

          - 4、He初始化

            - 1、正态化He初始化：tf.keras.initializers.he_normal()
            - 特性：以 0 为中心，标准差为 `stddev = sqrt(2 / fan_in)` 的截断正态分布中抽取样本
            - 2、标准化的He初始化：tf.keras.initializers.he_uniform()
            - 特点：它从 [-limit，limit] 中的均匀分布中抽取样本， 其中 `limit` 是 `sqrt(6 / fan_in)`
            - 论文中指出这样的初始化方法与RELU或者RELU系列的激活函数搭配使用效果更好

    - 模型的构建

      - 方式：1、通过Sequantial的方式去构建；2、通过继承Model抽象子类进行构建；3、通过函数式的方式构建