**卷积核大小对特征提取的影响**：

- 卷积核越大，提取特征后特征图的大小越小，忽略一些局部特征，得到的描述是全局的（宏观的）的特征，反之，提取的都是一些细节（局部）特征。

**神经网络的层数（深度）对模型的影响**：

- 网络层数越深，那么提取出的信息和特征越多，效果越好，假设空间越大，那么一定会存在一个全局最优解，只要我们不断的去训练模型，一定会找到这个最优解
- 图像内容比较简单，特征较少，那么可以使用浅层的网络去训练，反之，使用深层网络去训练，可能才会得到好的效果

**参数和超参数的区别**：

- 参数一般指的是随着模型的训练一块更新的一些参数，W， B， 批标准化中的 $$\gamma$$ 和 $$\beta$$ 。
- 超参数：指的是需要手动设置并调优的这些参数，学习率，正则化系数，网络的层数，卷积核的个数，卷积核的大小

感受野（receptived field）:

- 随着网络的加深，模型的感受野是越来越大的。在网络的浅层，模型提取的是一些细节特征（边界、纹理等），深层得到的是更加宏观的特征。

VGG：

- 模型中使用的都是一些比较小的卷积核，参数的量直接减少了，便于后面的计算

GoogLeNet：

- 使用了比较重要的结构：**inception**，这个结构中使用了大量的1X1的小卷积核，通过这样的方式，可以到达对输入特征进行降维的效果，增加了网络的宽度，缺点是忽略了特征之间的关联
- 2个辅助分类器，缓解梯度消失的状况，最后将3个结果做了一个加权，参与到最后的分类结果中，占的权重不一样
- 提升版本：
  - V2：将inception中的5X5卷积拆分成两个3X3的卷积，减少了参数量
  - V3：将3X3的卷积拆分成1X3和3X1的卷积，减少了参数的数量
  - V4：使用到了残差结构

ResNet（Residual Net）：

- 因为随着神经网路层数的加深，模型的表现不仅没有变好，反而变差了，发生了网络退化（不是由于梯度消失或者梯度爆炸引起的），而是模型在训练的过程中达到了饱和的状态

**图像增强**：

- 通过一些手段（翻转、旋转、裁切、仿射变换等），让数据集的表现更加丰富，同时扩增原始数据集
- tf.images或者tf.keras.imageGenerator

模型的微调（fine-tuning）:

- 预训练模型：在已知数据集上训练好的效果不错的模型
- 微调的方式：
  - 1、直接拿过来用，然后在现有模型的基础上添加自定义的全连接层，然后进行训练
  - 2、不是全部拿过来，只要主干部分，舍弃源模型后面的全连接层，然后自定义添加全连接层进行训练